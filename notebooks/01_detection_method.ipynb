{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e7e3c7-33af-43ad-92aa-04e2efbfa223",
   "metadata": {},
   "source": [
    "# Eddy Detection Notebook — Hybrid (OW + Nencioli + Closed Contours)\n",
    "\n",
    "**Project:** Eddy Detection in the Gulf of California (2010–2024)  \n",
    "**Purpose:** Detect cyclonic/anticyclonic eddies combining normalized Okubo–Weiss, Nencioli ring criterion, and closed-contour selection (Chelton et al., 2011), and export a tracking-ready dataset.  \n",
    "**Inputs:** `anomalias_GC_NeurOST_2010_2024_detrended_allvars.nc`  \n",
    "**Outputs:** `remolinos_completo_refinado2_2cm.nc` (eddy properties, masks, fields), optional figures for selected days.  \n",
    "**Reproducibility:** Make sure your environment matches the repository `environment.yml`. Set `SHOW_FIGS=True` if you want to render daily maps.\n",
    "\n",
    "**Notes**\n",
    "- This notebook is streamlined for the GitHub repo: English-only text, clear sectioning, and optional visualization.\n",
    "- Figures for selected days are meant as supplementary gallery (not necessarily in the paper).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202be2ec-2baa-46e5-a36b-8838b1553075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.ndimage import minimum_filter, binary_dilation\n",
    "from skimage.measure import label, regionprops\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Geod\n",
    "from scipy.signal import detrend\n",
    "from shapely.geometry import Polygon, Point\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "# --- Visualization switch\n",
    "SHOW_FIGS = False   # set True to export daily detection maps\n",
    "\n",
    "# --- 1) Global parameters (physical & algorithmic)\n",
    "Omega        = 7.2921e-5      # Earth rotation [rad s^-1]\n",
    "W_threshold  = -0.02          # floor threshold for candidate mask (normalized OW)\n",
    "b_param      = 3              # speed local-min filter radius (pixels)\n",
    "a_param      = 5              # Nencioli criterion radius (pixels)\n",
    "amp_thresh   = 0.025          # minimum amplitude (m)\n",
    "min_px       = 8              # minimum pixels inside contour\n",
    "max_px       = 1000           # maximum pixels inside contour\n",
    "max_dist_km  = 500            # maximum diameter (km) cap for closed contours\n",
    "\n",
    "# Haversine / geodesics\n",
    "geod = pyproj.Geod(ellps=\"WGS84\")\n",
    "\n",
    "# --- Portable grid parameters in kilometers (converted to pixels later)\n",
    "a_param_km   = 10.0   # Nencioli ring radius (8–15 km typical)\n",
    "b_param_km   = 8.0    # neighborhood for local speed minimum\n",
    "eps_speed_p  = 0.001  # fraction of p95(|u|) to relax strict minimum\n",
    "circ_min     = 0.50   # minimum circularity (CHE11-like filter)\n",
    "\n",
    "# --- Days to process (indexes in ds.time)\n",
    "DAYS_TO_PROCESS = [1, 5052, 32, 510, 5237, 1464, 2635, 3025, 1856, 250, 101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18364f97-474b-4b5c-9379-a0180212ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circularity(seg: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Circularity metric: 4π * area / perimeter^2, computed in km units.\n",
    "    Parameters\n",
    "    ----------\n",
    "    seg : (N,2) array of [lon, lat] forming a closed contour.\n",
    "    \"\"\"\n",
    "    lons, lats = seg[:, 0], seg[:, 1]\n",
    "    lon_km = (lons - lons.mean()) * 111 * np.cos(np.deg2rad(lats.mean()))\n",
    "    lat_km = (lats - lats.mean()) * 111\n",
    "    coords = np.column_stack((lon_km, lat_km))\n",
    "    poly = Polygon(coords)\n",
    "    per  = poly.length\n",
    "    area = poly.area\n",
    "    return (4 * np.pi * area / per**2) if per > 0 else 0.0\n",
    "\n",
    "\n",
    "def grid_spacing_km(lon_vec: np.ndarray, lat_vec: np.ndarray) -> tuple[float, float]:\n",
    "    \"\"\"Return typical grid spacing (dx_km, dy_km).\"\"\"\n",
    "    dlon = np.diff(lon_vec); dlat = np.diff(lat_vec)\n",
    "    lat0 = np.mean(lat_vec)\n",
    "    dx = np.median(np.abs(dlon)) * 111.0 * np.cos(np.deg2rad(lat0))\n",
    "    dy = np.median(np.abs(dlat)) * 111.0\n",
    "    return dx, dy\n",
    "\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2) -> float:\n",
    "    \"\"\"Great-circle distance between (lon1,lat1) and (lon2,lat2) in kilometers.\"\"\"\n",
    "    R = 6371.0\n",
    "    φ1, φ2 = np.deg2rad(lat1), np.deg2rad(lat2)\n",
    "    Δφ = np.deg2rad(lat2 - lat1)\n",
    "    Δλ = np.deg2rad(lon2 - lon1)\n",
    "    a = np.sin(Δφ/2)**2 + np.cos(φ1)*np.cos(φ2)*np.sin(Δλ/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "\n",
    "def generate_eddy_mask(lat_vec, lon_vec, contours):\n",
    "    \"\"\"Boolean mask True inside any provided contour segments.\"\"\"\n",
    "    ny, nx = len(lat_vec), len(lon_vec)\n",
    "    mask = np.zeros((ny, nx), dtype=bool)\n",
    "    LON, LAT = np.meshgrid(lon_vec, lat_vec)\n",
    "    pts = np.vstack((LON.ravel(), LAT.ravel())).T\n",
    "    for seg in contours:\n",
    "        path = Path(np.column_stack((seg[:, 0], seg[:, 1])))\n",
    "        mask |= path.contains_points(pts).reshape((ny, nx))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def centroid_from_mask(mask: np.ndarray, lon_vec, lat_vec) -> tuple[float, float]:\n",
    "    \"\"\"Return centroid (lat, lon) of a boolean mask.\"\"\"\n",
    "    ys, xs = np.where(mask)\n",
    "    return (lat_vec[ys].mean(), lon_vec[xs].mean())\n",
    "\n",
    "\n",
    "def eddy_stat(mask_bool: np.ndarray, field: np.ndarray) -> float:\n",
    "    \"\"\"Masked mean of `field`.\"\"\"\n",
    "    return np.nanmean(field[mask_bool])\n",
    "\n",
    "\n",
    "def centroid_core_sla(sla, lat_vec, lon_vec, seg, cyclonic=True) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Core centroid based on SLA extreme inside polygon.\n",
    "    cyclonic=True -> minimum SLA; else maximum SLA.\n",
    "    \"\"\"\n",
    "    poly = Polygon(seg)  # seg: Nx2 [[lon,lat],...]\n",
    "    LON, LAT = np.meshgrid(lon_vec, lat_vec)\n",
    "    points = np.vstack([LON.ravel(), LAT.ravel()]).T\n",
    "    mask_int = np.array([poly.contains(Point(p)) for p in points]).reshape(LON.shape)\n",
    "    vals = sla[mask_int]\n",
    "    if vals.size == 0:\n",
    "        c = poly.centroid\n",
    "        return c.y, c.x\n",
    "    target = vals.min() if cyclonic else vals.max()\n",
    "    i0, j0 = np.where(sla == target)\n",
    "    return lat_vec[i0[0]], lon_vec[j0[0]]\n",
    "\n",
    "\n",
    "def diameter_from_seg(seg, lat_c, lon_c) -> float:\n",
    "    \"\"\"Diameter as 2 × median radius from (lat_c, lon_c) to contour vertices (km).\"\"\"\n",
    "    radios = []\n",
    "    for lon, lat in seg:\n",
    "        _, _, d = geod.inv(lon_c, lat_c, lon, lat)\n",
    "        radios.append(d / 1000.0)\n",
    "    return 2.0 * np.median(radios)\n",
    "\n",
    "\n",
    "def axes_from_pca_on_seg(seg) -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Major/minor axes and eccentricity from PCA over the contour (in km).\n",
    "    Returns (major_km, minor_km, ecc).\n",
    "    \"\"\"\n",
    "    lons, lats = seg[:, 0], seg[:, 1]\n",
    "    lon0 = np.mean(lons); lat0 = np.mean(lats)\n",
    "    x = (lons - lon0) * 111.0 * np.cos(np.deg2rad(lat0))\n",
    "    y = (lats - lat0) * 111.0\n",
    "    X = np.column_stack([x, y])\n",
    "    C = np.cov(X, rowvar=False)\n",
    "    w, _ = np.linalg.eigh(C)\n",
    "    w = np.sort(w)[::-1]\n",
    "    major = 2.0 * np.sqrt(max(w[0], 0.0))\n",
    "    minor = 2.0 * np.sqrt(max(w[1], 0.0))\n",
    "    ecc = np.sqrt(1 - (minor/major)**2) if major > 0 and minor > 0 else 0.0\n",
    "    return major, minor, ecc\n",
    "\n",
    "\n",
    "def nencioli_ring(mask, u, v, a_pix: int, min_ok=6, alpha=0.7) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Nencioli ring criterion:\n",
    "    - Tangential component vt consistent in sign (>= min_ok of 8 directions).\n",
    "    - Radial component bounded: |vr| < alpha * |vt|.\n",
    "    - vt on ring > vt at center.\n",
    "    \"\"\"\n",
    "    ny, nx = mask.shape\n",
    "    refined = np.zeros_like(mask, dtype=bool)\n",
    "    dirs = [(0,1),(1,1),(1,0),(1,-1),(0,-1),(-1,-1),(-1,0),(-1,1)]\n",
    "    for i in range(a_pix, ny - a_pix):\n",
    "        for j in range(a_pix, nx - a_pix):\n",
    "            if not mask[i, j]:\n",
    "                continue\n",
    "            vt_signs = []\n",
    "            ok = 0\n",
    "            for dy, dx in dirs:\n",
    "                ii = i + dy * a_pix\n",
    "                jj = j + dx * a_pix\n",
    "                rr = np.array([dx, dy], dtype=float)\n",
    "                rr /= np.hypot(rr[0], rr[1])\n",
    "                t_hat = np.array([-rr[1], rr[0]])  # 90° rotation\n",
    "                vt = u[ii, jj] * t_hat[0] + v[ii, jj] * t_hat[1]\n",
    "                vr = u[ii, jj] * rr[0] + v[ii, jj] * rr[1]\n",
    "                vt_c = u[i, j]  * t_hat[0] + v[i, j]  * t_hat[1]\n",
    "                if (abs(vt) > abs(vt_c)) and (abs(vr) < alpha * abs(vt)):\n",
    "                    vt_signs.append(np.sign(vt) if vt != 0 else 0)\n",
    "                    ok += 1\n",
    "            if ok >= min_ok and len(vt_signs) > 0:\n",
    "                if abs(np.sum(vt_signs)) >= (min_ok - 1):  # allow at most 1 mismatch\n",
    "                    refined[i, j] = True\n",
    "    return refined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb609c4c-0a87-49a3-b623-12324ef3550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Open detrended NetCDF and build structures\n",
    "ds = xr.open_dataset(\"anomalias_GC_NeurOST_2010_2024_detrended_allvars.nc\")\n",
    "\n",
    "lat_vec = ds.lat.values\n",
    "lon_vec = ds.lon.values\n",
    "dx_km, dy_km = grid_spacing_km(lon_vec, lat_vec)\n",
    "pix_km = np.mean([dx_km, dy_km])\n",
    "a_pix = max(1, int(round(a_param_km / pix_km)))\n",
    "b_pix = max(1, int(round(b_param_km / pix_km)))\n",
    "\n",
    "Lon2d, Lat2d = np.meshgrid(lon_vec, lat_vec)\n",
    "pts_flat = np.column_stack((Lon2d.ravel(), Lat2d.ravel()))\n",
    "\n",
    "n_time = ds.time.size\n",
    "mask_3d = np.zeros((n_time, len(lat_vec), len(lon_vec)), dtype=bool)\n",
    "\n",
    "# per-day detections (kept in a Python dict for flexible post-processing)\n",
    "eddies_by_day = {ts.item(): [] for ts in ds.time.values}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65af4e3-926f-413f-8441-be5d4b8dfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Detection loop — selected days\n",
    "for t_idx in tqdm(DAYS_TO_PROCESS, desc=\"Processing selected days\", unit=\"day\"):\n",
    "    ts = ds.time.values[t_idx]\n",
    "    date = pd.to_datetime(ts)\n",
    "\n",
    "    # --- Fields for day t\n",
    "    sla   = ds[\"sla_dtrend2d\"].isel(time=t_idx).values\n",
    "    sn    = ds[\"sn_dtrend2d\"].isel(time=t_idx).values\n",
    "    ss    = ds[\"ss_dtrend2d\"].isel(time=t_idx).values\n",
    "    zeta  = ds[\"zeta_dtrend2d\"].isel(time=t_idx).values\n",
    "    ugos  = ds[\"ugosa_dtrend2d\"].isel(time=t_idx).values\n",
    "    vgos  = ds[\"vgosa_dtrend2d\"].isel(time=t_idx).values\n",
    "\n",
    "    # --- Stage 1: HAL13 (normalized Okubo–Weiss) + local speed minimum\n",
    "    W = sn**2 + ss**2 - zeta**2\n",
    "    f = 2 * Omega * np.sin(np.deg2rad(lat_vec))\n",
    "    Wn = W / (f[:, None]**2)\n",
    "\n",
    "    # dynamic threshold: p10, bounded by W_threshold (more restrictive)\n",
    "    thr_dyn = np.nanpercentile(Wn, 10)\n",
    "    thr = min(W_threshold, thr_dyn)\n",
    "    candidate = Wn < thr\n",
    "\n",
    "    speed = np.sqrt(ugos**2 + vgos**2)\n",
    "    eps = eps_speed_p * np.nanpercentile(speed, 95)\n",
    "    local_min = speed <= (minimum_filter(speed, size=b_pix) + eps)\n",
    "\n",
    "    halo_mask = binary_dilation(candidate & local_min,\n",
    "                                structure=np.ones((2*b_pix+1, 2*b_pix+1)))\n",
    "\n",
    "    # --- Stage 2: Nencioli\n",
    "    refined_mask = nencioli_ring(halo_mask, ugos, vgos, a_pix=a_pix, min_ok=6, alpha=0.7)\n",
    "\n",
    "    # --- Stage 3: CHE11-like closed contours + your post-filters\n",
    "\n",
    "    # 3.1) SLA contours (no masks at this step)\n",
    "    vmax = float(np.nanmax(np.abs(sla)))\n",
    "    levels = np.linspace(-vmax, vmax, 50)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    cs = ax.contour(lon_vec, lat_vec, sla, levels=levels)\n",
    "    levels_cs, allsegs = cs.levels, cs.allsegs\n",
    "    plt.close(fig)\n",
    "\n",
    "    # 3.2) Basic circularity filter (closed, >=3 points)\n",
    "    conts = []\n",
    "    for lvl, segs in zip(levels_cs, allsegs):\n",
    "        for seg in segs:\n",
    "            if seg.shape[0] < 3 or not np.allclose(seg[0], seg[-1], atol=1e-3):\n",
    "                continue\n",
    "            if circularity(seg) >= 0.6:\n",
    "                conts.append((lvl, seg))\n",
    "\n",
    "    # 3.2a) Remove “deformed parents” (your original logic)\n",
    "    circs = [circularity(s) for _, s in conts]\n",
    "    kept = []\n",
    "    for i, (lvl_i, seg_i) in enumerate(conts):\n",
    "        pi, drop = Path(seg_i), False\n",
    "        for j, (_, seg_j) in enumerate(conts):\n",
    "            if i == j: \n",
    "                continue\n",
    "            if pi.contains_path(Path(seg_j)) and circs[j] > circs[i] * 1.2:\n",
    "                drop = True\n",
    "                break\n",
    "        if not drop:\n",
    "            kept.append((lvl_i, seg_i))\n",
    "    conts = kept\n",
    "\n",
    "    # 3.2b) Group nested contours (your logic)\n",
    "    groups, used = [], set()\n",
    "    for i, (lvl_i, seg_i) in enumerate(conts):\n",
    "        if i in used:\n",
    "            continue\n",
    "        Pi, fam = Path(seg_i), [(lvl_i, seg_i)]\n",
    "        used.add(i)\n",
    "        for j, (lvl_j, seg_j) in enumerate(conts):\n",
    "            if j in used:\n",
    "                continue\n",
    "            if Pi.contains_path(Path(seg_j)):\n",
    "                fam.append((lvl_j, seg_j))\n",
    "                used.add(j)\n",
    "        groups.append(fam)\n",
    "\n",
    "    # Select best per family by amplitude vs. edge median (>= amp_thresh)\n",
    "    selected_segs = []\n",
    "    for fam in groups:\n",
    "        best_A, best_seg = -np.inf, None\n",
    "        for lvl, seg in fam:\n",
    "            path = Path(seg)\n",
    "            mask_int = path.contains_points(pts_flat).reshape(sla.shape)\n",
    "            if not mask_int.any():\n",
    "                continue\n",
    "            idx_i = [np.argmin(abs(lat_vec - lat)) for _, lat in seg]\n",
    "            idx_j = [np.argmin(abs(lon_vec - lon)) for lon, _ in seg]\n",
    "            h0 = sla[idx_i, idx_j].mean()\n",
    "            A  = max(np.nanmax(sla[mask_int]) - h0, h0 - np.nanmin(sla[mask_int]))\n",
    "            if A > best_A:\n",
    "                best_A, best_seg = A, seg\n",
    "        if best_seg is not None and best_A >= amp_thresh:\n",
    "            selected_segs.append(best_seg)\n",
    "\n",
    "    # Final contour filtering (size, amplitude, max diameter)\n",
    "    final_segs = []\n",
    "    mask_contours = np.zeros_like(sla, dtype=bool)\n",
    "    for seg in selected_segs:\n",
    "        path = Path(seg)\n",
    "        mask_int = path.contains_points(pts_flat).reshape(sla.shape)\n",
    "        if mask_int.sum() < min_px:\n",
    "            continue\n",
    "        idx_i = [np.argmin(abs(lat_vec - lat)) for _, lat in seg]\n",
    "        idx_j = [np.argmin(abs(lon_vec - lon)) for lon, _ in seg]\n",
    "        h0 = sla[idx_i, idx_j].mean()\n",
    "        A  = max(np.nanmax(sla[mask_int]) - h0, h0 - np.nanmin(sla[mask_int]))\n",
    "        if A < amp_thresh:\n",
    "            continue\n",
    "        hull_pts = seg[ConvexHull(seg).vertices]\n",
    "        dmax = max(haversine(p1[0], p1[1], p2[0], p2[1]) for p1 in hull_pts for p2 in hull_pts)\n",
    "        if dmax > max_dist_km:\n",
    "            continue\n",
    "        final_segs.append(seg)\n",
    "        mask_contours |= mask_int\n",
    "\n",
    "    # “Leftover bumps”: keep coherent blobs from refined_mask outside contours\n",
    "    leftover = refined_mask & (~mask_contours)\n",
    "    for prop in regionprops(label(leftover)):\n",
    "        if prop.area < 10:\n",
    "            continue\n",
    "        ys, xs = prop.coords[:, 0], prop.coords[:, 1]\n",
    "        pts2 = np.column_stack((lon_vec[xs], lat_vec[ys]))\n",
    "        try:\n",
    "            seg2 = pts2[ConvexHull(pts2).vertices]\n",
    "            final_segs.append(seg2)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Remove redundant nested contours\n",
    "    filtered_segs = [seg for seg in final_segs\n",
    "        if not any(Path(o).contains_path(Path(seg)) and not np.array_equal(o, seg)\n",
    "                   for o in final_segs)]\n",
    "\n",
    "    # Deduplicate by centroid distance\n",
    "    tol_km = 10.0\n",
    "    final_contours = []\n",
    "    seen_centroids = []\n",
    "    for seg in filtered_segs:\n",
    "        mask_e = Path(seg).contains_points(pts_flat).reshape(sla.shape)\n",
    "        lat_c, lon_c = centroid_from_mask(mask_e, lon_vec=lon_vec, lat_vec=lat_vec)\n",
    "        if all(haversine(lon_c, lat_c, lon0, lat0) > tol_km for lon0, lat0 in seen_centroids):\n",
    "            final_contours.append(seg)\n",
    "            seen_centroids.append((lon_c, lat_c))\n",
    "\n",
    "    # Build 3D eddy mask\n",
    "    mask_3d[t_idx] = generate_eddy_mask(lat_vec, lon_vec, final_contours)\n",
    "\n",
    "    # Extract properties\n",
    "    eddies = []\n",
    "    for seg in final_contours:\n",
    "        mask_e = Path(seg).contains_points(pts_flat).reshape(sla.shape)\n",
    "        vort = eddy_stat(mask_e, zeta)\n",
    "        ug   = eddy_stat(mask_e, ugos)\n",
    "        vg   = eddy_stat(mask_e, vgos)\n",
    "        eddy_type = \"Cyclonic\" if vort > 0 else \"Anticyclonic\"\n",
    "        cyclonic = (eddy_type == \"Cyclonic\")\n",
    "        lat_c, lon_c = centroid_core_sla(sla, lat_vec, lon_vec, seg, cyclonic=cyclonic)\n",
    "        diameter_km = diameter_from_seg(seg, lat_c, lon_c)\n",
    "        major, minor, ecc = axes_from_pca_on_seg(seg)\n",
    "\n",
    "        eddies.append({\n",
    "            \"time\":         date,\n",
    "            \"centroid\":     (lat_c, lon_c),\n",
    "            \"major_axis\":   major,\n",
    "            \"minor_axis\":   minor,\n",
    "            \"diameter\":     diameter_km,\n",
    "            \"eccentricity\": ecc,\n",
    "            \"vorticity\":    vort,\n",
    "            \"ugos\":         ug,\n",
    "            \"vgos\":         vg,\n",
    "            \"type\":         eddy_type,\n",
    "            \"contour\":      seg,\n",
    "        })\n",
    "\n",
    "    eddies_by_day[ts.item()] = eddies\n",
    "\n",
    "    # Optional visualization for gallery\n",
    "    if SHOW_FIGS:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "        cf = ax.contourf(lon_vec, lat_vec, sla, levels=20, cmap=\"RdBu_r\",\n",
    "                         transform=ccrs.PlateCarree(), zorder=1)\n",
    "        ax.coastlines(resolution='10m', zorder=2)\n",
    "        ax.add_feature(cfeature.LAND, color='black', zorder=2)\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':', zorder=2)\n",
    "\n",
    "        gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5, x_inline=False, y_inline=False)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        gl.xformatter = LONGITUDE_FORMATTER\n",
    "        gl.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "        for eddy in eddies:\n",
    "            seg = eddy[\"contour\"]\n",
    "            lat_c, lon_c = eddy[\"centroid\"]\n",
    "            radius_km = eddy[\"diameter\"] / 2\n",
    "            ax.plot(seg[:, 0], seg[:, 1], '--', linewidth=2,\n",
    "                    transform=ccrs.PlateCarree(), zorder=3)\n",
    "            # draw a reference circle with same diameter\n",
    "            θ = np.linspace(0, 2*np.pi, 200)\n",
    "            dx = radius_km * np.cos(θ)\n",
    "            dy = radius_km * np.sin(θ)\n",
    "            lon_circ = lon_c + dx / (111.0 * np.cos(np.deg2rad(lat_c)))\n",
    "            lat_circ = lat_c + dy / 111.0\n",
    "            ax.plot(lon_circ, lat_circ, ':', lw=1, transform=ccrs.PlateCarree(), zorder=3)\n",
    "            ax.plot(lon_c, lat_c, 'x', transform=ccrs.PlateCarree(), zorder=4)\n",
    "\n",
    "        cbar = fig.colorbar(cf, ax=ax, orientation='vertical', pad=0.02)\n",
    "        cbar.set_label(\"SSHA (m)\")\n",
    "        ax.set_title(f\"Detected Eddies — idx {t_idx} ({date.date()})\")\n",
    "        ax.set_xlabel(\"Longitude\")\n",
    "        ax.set_ylabel(\"Latitude\")\n",
    "        plt.show()\n",
    "\n",
    "    # Console summary\n",
    "    print(f\"[{date.date()}] final_contours: {len(final_contours)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b42503-38b1-4e7c-b310-d5316824d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Build tracking-ready Dataset (keeps your original variable names where sensible)\n",
    "\n",
    "max_eddies = max(len(v) for v in eddies_by_day.values())\n",
    "times      = ds.time.values\n",
    "eddy_idx   = np.arange(max_eddies)\n",
    "\n",
    "cent_lat  = np.full((n_time, max_eddies), np.nan)\n",
    "cent_lon  = np.full((n_time, max_eddies), np.nan)\n",
    "diam_km   = np.full((n_time, max_eddies), np.nan)\n",
    "max_km    = np.full((n_time, max_eddies), np.nan)\n",
    "min_km    = np.full((n_time, max_eddies), np.nan)\n",
    "ecc_arr   = np.full((n_time, max_eddies), np.nan)\n",
    "vort_arr  = np.full((n_time, max_eddies), np.nan)\n",
    "ug_arr    = np.full((n_time, max_eddies), np.nan)\n",
    "vg_arr    = np.full((n_time, max_eddies), np.nan)\n",
    "type_arr  = np.full((n_time, max_eddies), \"\", dtype=object)\n",
    "\n",
    "for i, ts in enumerate(times):\n",
    "    for j, eddy in enumerate(eddies_by_day[ts.item()]):\n",
    "        cent_lat[i, j] = eddy[\"centroid\"][0]\n",
    "        cent_lon[i, j] = eddy[\"centroid\"][1]\n",
    "        max_km[i, j]   = eddy[\"major_axis\"]\n",
    "        min_km[i, j]   = eddy[\"minor_axis\"]\n",
    "        diam_km[i, j]  = eddy[\"diameter\"]\n",
    "        ecc_arr[i, j]  = eddy[\"eccentricity\"]\n",
    "        vort_arr[i, j] = eddy[\"vorticity\"]\n",
    "        ug_arr[i, j]   = eddy[\"ugos\"]\n",
    "        vg_arr[i, j]   = eddy[\"vgos\"]\n",
    "        type_arr[i, j] = eddy[\"type\"]\n",
    "\n",
    "ds_eddies = xr.Dataset(\n",
    "    {\n",
    "        \"centroid_lat\":  ([\"time\",\"eddy\"], cent_lat),\n",
    "        \"centroid_lon\":  ([\"time\",\"eddy\"], cent_lon),\n",
    "        \"diameter_km\":   ([\"time\",\"eddy\"], diam_km),\n",
    "        \"major_axis_km\": ([\"time\",\"eddy\"], max_km),\n",
    "        \"minor_axis_km\": ([\"time\",\"eddy\"], min_km),\n",
    "        \"eccentricity\":  ([\"time\",\"eddy\"], ecc_arr),\n",
    "        \"vorticity\":     ([\"time\",\"eddy\"], vort_arr),\n",
    "        \"ugos\":          ([\"time\",\"eddy\"], ug_arr),\n",
    "        \"vgos\":          ([\"time\",\"eddy\"], vg_arr),\n",
    "        \"type\":          ([\"time\",\"eddy\"], type_arr),\n",
    "        \"mask_eddies\":   ([\"time\",\"lat\",\"lon\"], mask_3d),\n",
    "        # raw (non-detrended) anomaly products for downstream checks:\n",
    "        \"ssha\":          ([\"time\",\"lat\",\"lon\"], ds.sla_anomaly.values),\n",
    "        \"okubo_weiss\":   ([\"time\",\"lat\",\"lon\"],\n",
    "                           ds.sn_anomaly.values**2 +\n",
    "                           ds.ss_anomaly.values**2 -\n",
    "                           ds.zeta_anomaly.values**2),\n",
    "    },\n",
    "    coords={\"time\": ds.time.values, \"lat\": lat_vec, \"lon\": lon_vec, \"eddy\": eddy_idx},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e32e2b-d2a7-4d34-bd1b-36cb850befbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Save to NetCDF\n",
    "out_path = \"remolinos_completo_refinado2_2cm.nc\"\n",
    "ds_eddies.to_netcdf(out_path)\n",
    "print(f\"✅ Saved: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
